{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Submission notebook for Severstal: Steel Defect Detection\n",
    "https://www.kaggle.com/c/severstal-steel-defect-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = '/kaggle/input/severstal-steel-defect-detection/test_images'\n",
    "MODEL_PATH = '/kaggle/input/unetd-model-weights/unetd_D5F48E20_final.pth'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mean = (0.3438812517320017,)\n",
    "img_std = (0.13965334396720055,)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=img_mean, std=img_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FRAMES = 7\n",
    "FRAME_SIZE = (256, 256)\n",
    "IMG_SIZE = (256, 1600)\n",
    "OVERLAP = (NUM_FRAMES * FRAME_SIZE[1] - IMG_SIZE[1]) // (NUM_FRAMES - 1)\n",
    "\n",
    "class FramesDataset(Dataset):\n",
    "    \"\"\"Severstal kaggle competition dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 datadir,\n",
    "                 imglist=None,\n",
    "                 transform=T.ToTensor()):\n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imglist = os.listdir(datadir)\n",
    "        self.imglist.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return NUM_FRAMES * len(self.imglist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_idx = index // NUM_FRAMES\n",
    "        frame_idx = index % NUM_FRAMES\n",
    "        left = frame_idx * (FRAME_SIZE[1] - OVERLAP)\n",
    "\n",
    "        fname = self.imglist[img_idx]\n",
    "        img = Image.open(os.path.join(self.datadir, fname)).convert(mode='L')\n",
    "        # For all input images with R == G == B. Checked\n",
    "\n",
    "        img = self.transform(img)\n",
    "        img = img[:, :, left:left+FRAME_SIZE[1]] # Crop frame\n",
    "\n",
    "        return img, fname\n",
    "\n",
    "ds = FramesDataset(TEST_PATH, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_loader = {\n",
    "    'batch_size': NUM_FRAMES,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "# Data loaders\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    shuffle=False,\n",
    "    **params_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    \"\"\"Swish activation function by Google\n",
    "    $Swish = x * \\sigma(x)$\n",
    "    \"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"Swish activation function by Google\n",
    "    $Swish = x * \\sigma(x)$\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return swish(x)\n",
    "\n",
    "\n",
    "activations = {\n",
    "    'relu': F.relu,\n",
    "    'swish': swish\n",
    "    }\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, pad=0, bn=False, activation='relu'):\n",
    "        \"\"\"\n",
    "        Convolutional block of U-net architecture without final activation\n",
    "        (it is optimal to make ReLU after max pool)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bn = bn\n",
    "        self.activation = activations[activation]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel,\n",
    "                               (3, 3), padding=pad, bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel,\n",
    "                               (3, 3), padding=pad, bias=True)\n",
    "\n",
    "        if self.bn:\n",
    "            self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "            self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.bn: x = self.bn1(x)\n",
    "        x = self.conv2(self.activation(x))\n",
    "        if self.bn: x = self.bn2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Up convolution on the way up\n",
    "    Accepts input x from previouse layer and concatenates output with\n",
    "    features f from down pass\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channel, in_channel // 2,\n",
    "                                         (2, 2), stride=2, bias=True)\n",
    "    \n",
    "    def forward(self, x, f):\n",
    "        x = self.upconv(F.relu(x))\n",
    "        # do we need relu for x here?\n",
    "        out = F.relu(torch.cat([f, x], dim=1))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UnetD(nn.Module):\n",
    "    \"\"\"Unet with custom depth D\n",
    "    \"\"\"\n",
    "    def __init__(self, depth, n_filters, bn=False, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "\n",
    "        self.activation = activations[activation]\n",
    "\n",
    "        # down\n",
    "        self.dn_blks = nn.ModuleList()\n",
    "        in_ch = 1\n",
    "        out_ch = n_filters\n",
    "        for dd in range(self.depth):\n",
    "            self.dn_blks.append(ConvBlock(in_ch, out_ch, pad=1, bn=bn, activation=activation))\n",
    "            in_ch = out_ch\n",
    "            out_ch *= 2\n",
    "\n",
    "        # bottom\n",
    "        self.bottom = ConvBlock(in_ch, out_ch, pad=1, bn=bn, activation=activation)\n",
    "        in_ch, out_ch = out_ch, in_ch\n",
    "\n",
    "        # up\n",
    "        self.upconvs = nn.ModuleList()\n",
    "        self.up_blks = nn.ModuleList()\n",
    "        for dd in range(self.depth):\n",
    "            self.upconvs.append(UpPool(in_ch))\n",
    "            self.up_blks.append(ConvBlock(in_ch, out_ch, pad=1, bn=bn, activation=activation))\n",
    "            in_ch = out_ch\n",
    "            out_ch = out_ch // 2\n",
    "\n",
    "        # output\n",
    "        self.outconv = nn.Conv2d(n_filters, 5, (1, 1), bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for dn_blk in self.dn_blks:\n",
    "            x = dn_blk(x)\n",
    "            outs.append(x)\n",
    "            x = self.activation(F.max_pool2d(x, (2, 2)))\n",
    "\n",
    "        x = self.bottom(x)\n",
    "        outs.reverse()\n",
    "\n",
    "        for upconv, up_blk, out in zip(self.upconvs, self.up_blks, outs):\n",
    "            x = up_blk(upconv(x, out))\n",
    "\n",
    "        x = self.outconv(self.activation(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_unet = {\n",
    "    'depth': 5,\n",
    "    'n_filters': 48,\n",
    "    'bn': True,\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "unet = UnetD(**params_unet)\n",
    "unet = unet.to(device=device);\n",
    "\n",
    "# Load weights\n",
    "unet.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "> ## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    \"\"\"Encode image mask to run length encoding string\n",
    "    \"\"\"\n",
    "    dots = np.where(mask.T.flatten() == 1)[0] # .T for Fortran order (down then right)\n",
    "    rle = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            rle.extend((b + 1, 0))\n",
    "        rle[-1] += 1\n",
    "        prev = b\n",
    "\n",
    "    rle = ' '.join(map(str, rle))\n",
    "\n",
    "    return rle\n",
    "\n",
    "def combine_frames(scores):\n",
    "    \"\"\"Combine scores with batch size of 7 frames to predicted image\n",
    "    \"\"\"\n",
    "    assert scores.shape == (7, 5, 256, 256), f\"Input tensor shape {scores.shape}, while (7, 5, 256, 256) expected\"\n",
    "    \n",
    "    scores = scores.cpu()\n",
    "    \n",
    "    out = torch.zeros(5, 256, 1600)\n",
    "    for ii in range(scores.shape[0]):\n",
    "        left = ii * 224\n",
    "        out[:, :, left:left+256] += scores[ii, :, :, :]\n",
    "        if left > 0:\n",
    "            out[left : left+32] /= 2\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5506/5506 [20:30<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 45s, sys: 2min 5s, total: 19min 50s\n",
      "Wall time: 20min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def gen_submission(model, loader):\n",
    "    model.eval()\n",
    "    \n",
    "    with open('submission.csv', mode='w') as submission:\n",
    "        submission.write('ImageId_ClassId,EncodedPixels\\n')\n",
    "        with torch.no_grad():\n",
    "            for x, fname in tqdm(loader):\n",
    "                x = x.to(device)\n",
    "                fname = fname[0]\n",
    "                \n",
    "                scores = model(x)\n",
    "                pred = combine_frames(scores)\n",
    "                pred = torch.argmax(pred, dim=0)\n",
    "                pred = pred.cpu().numpy()\n",
    "                for defect_type in range(1, 5):\n",
    "                    im_cls = '_'.join([fname, str(defect_type)])\n",
    "                    rle = rle_encode(pred == defect_type)\n",
    "                    submission.write(f\"{im_cls},{rle}\\n\")\n",
    "\n",
    "gen_submission(unet, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
